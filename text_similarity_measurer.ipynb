{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bag of Words:\n",
      "Logistic Regression Percent Correct:  91.82242990654206\n",
      "K Nearest Neighbor Percent Correct:  91.58878504672897\n",
      "Gaussian Naive Bayes Percent Correct:  77.33644859813083\n",
      "Support Vector Machine Percent Correct:  93.69158878504673\n",
      "Linear SVC Percent Correct:  83.8785046728972\n",
      "Linear Discriminant Analysis Percent Correct:  90.42056074766354\n",
      "Multi-Layer Perceptron Percent Correct:  92.5233644859813\n",
      "\n",
      "\n",
      "Accuracy of all of the above combined:  92.99065420560748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "<ipython-input-1-0a66c6f4d83a>:115: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combodf['sum'] = sumdf\n",
      "<ipython-input-1-0a66c6f4d83a>:116: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combodf['sumpreds'] = np.where(combodf['sum'] >= 3, True, False)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#open file\n",
    "train = pd.read_csv(r'train.csv')\n",
    "\n",
    "#Look at first 5 lines of data:\n",
    "#train.head()\n",
    "\n",
    "#Merge x text and y text together so I can put them in a matrix\n",
    "train['descriptions'] = train['description_x'].str.cat(train['description_y'],sep=\" \")\n",
    "\n",
    "#making vectors\n",
    "count_vectorizer = CountVectorizer(ngram_range = (1, 1)) \n",
    "#(1, 1 means unigrams; 2, 2 for bigrams, 3, 3 for trigrams)\n",
    "#Note: if you change the above, restart the kernel before running again or it won't come out right\n",
    "\n",
    "tokenlinelist = train['descriptions'][0:1713] #split data -- ratio 80:20\n",
    "X_train = count_vectorizer.fit_transform(tokenlinelist) #bag of words\n",
    "y_train = train['same_security'][0:1713]\n",
    "\n",
    "#split test set\n",
    "tokenlinelist_test = train['descriptions'][1714:2142]\n",
    "X_test = count_vectorizer.transform(tokenlinelist_test)\n",
    "y_test = train['same_security'][1714:2142]\n",
    "\n",
    "#Logistic Regresion:\n",
    "#train my LR\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "#Make LR predictions\n",
    "LRpreds = clf.predict(X_test)\n",
    "#print(LRpreds)\n",
    "\n",
    "#KNN classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "knc = neigh.predict(X_test)\n",
    "#knc\n",
    "\n",
    "#Gaussian Naive Bayes Classifier\n",
    "gnb = GaussianNB()\n",
    "gnbpreds = gnb.fit(X_train.toarray(), y_train).predict(X_test.toarray())\n",
    "#gnbpreds\n",
    "\n",
    "#SVM Classifier:\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svmpreds = svm.predict(X_test)\n",
    "\n",
    "#LinearSVC\n",
    "lsvc = make_pipeline(StandardScaler(with_mean = False), LinearSVC(random_state=0, tol=1e-5)) \n",
    "#had to add with_mean = False because I'm working with sparse matrix\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "lsvcpreds = lsvc.predict(X_test)\n",
    "\n",
    "#Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train.toarray(), y_train) #had to add .toarray() because sparse matrix\n",
    "\n",
    "ldapreds = lda.predict(X_test)\n",
    "\n",
    "#Multi-Layer Perceptron \n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=333)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlppreds = mlp.predict(X_test)\n",
    "\n",
    "#Wrangling predictions into one dataframe\n",
    "#Contains test set true classes and my various predicted classes\n",
    "predsdf = pd.DataFrame(LRpreds)\n",
    "truedf = pd.DataFrame(y_test)\n",
    "truedf = truedf.reset_index()\n",
    "knndf = pd.DataFrame(knc)\n",
    "gnbdf = pd.DataFrame(gnbpreds)\n",
    "lsvcdf = pd.DataFrame(lsvcpreds)\n",
    "predsdf['true'] = truedf['same_security']\n",
    "predsdf = predsdf.rename(columns={0: 'LRpreds'})\n",
    "knndf = knndf.rename(columns={0:'KNNpreds'})\n",
    "svmdf = pd.DataFrame(svmpreds)\n",
    "ldadf = pd.DataFrame(ldapreds)\n",
    "mlpdf = pd.DataFrame(mlppreds)\n",
    "predsdf['LRcorrect'] = predsdf['LRpreds'] == predsdf['true']\n",
    "predsdf['KNNpreds'] = knndf\n",
    "predsdf['KNNcorrect'] = predsdf['KNNpreds'] == predsdf['true']\n",
    "predsdf['GNBpreds'] = gnbdf\n",
    "predsdf['GNBcorrect'] = predsdf['GNBpreds'] == predsdf['true']\n",
    "predsdf['SVMpreds'] = svmdf\n",
    "predsdf['SVMcorrect'] = predsdf['SVMpreds'] == predsdf['true']\n",
    "predsdf['LinearSVCpreds'] = lsvcdf\n",
    "predsdf['LinearSVCcorrect'] = predsdf['LinearSVCpreds'] == predsdf['true']\n",
    "predsdf['LDApreds'] = ldadf\n",
    "predsdf['LDAcorrect'] = predsdf['LDApreds'] == predsdf['true']\n",
    "predsdf['MLPpreds'] = mlpdf\n",
    "predsdf['MLPcorrect'] = predsdf['MLPpreds'] == predsdf['true']\n",
    "#predsdf.head()\n",
    "\n",
    "#combining all prediction measures together:\n",
    "#If more than half the measures say true, then make a true prediction; same applies to false\n",
    "combodf = predsdf[['KNNpreds', 'GNBpreds', 'SVMpreds', 'LinearSVCpreds', 'LDApreds', 'MLPpreds']]\n",
    "combosumpreds = combodf.sum(axis=1)\n",
    "sumdf = pd.DataFrame(combosumpreds)\n",
    "combodf['sum'] = sumdf\n",
    "combodf['sumpreds'] = np.where(combodf['sum'] >= 3, True, False)\n",
    "#combodf\n",
    "df7 = combodf.join(predsdf['true'])\n",
    "df7['sumpredscorrect'] = df7['sumpreds'] == df7['true']\n",
    "#df7\n",
    "#print(df7['sumpredscorrect'])\n",
    "\n",
    "\n",
    "print('Results for Bag of Words:')\n",
    "print(\"Logistic Regression Percent Correct: \", predsdf.LRcorrect.sum()/len(predsdf)*100)\n",
    "print(\"K Nearest Neighbor Percent Correct: \", predsdf.KNNcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Gaussian Naive Bayes Percent Correct: \", predsdf.GNBcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Support Vector Machine Percent Correct: \", predsdf.SVMcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Linear SVC Percent Correct: \", predsdf.LinearSVCcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Linear Discriminant Analysis Percent Correct: \", predsdf.LDAcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Multi-Layer Perceptron Percent Correct: \", predsdf.MLPcorrect.sum()/len(predsdf)*100)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of all of the above combined: \", df7.sumpredscorrect.sum()/len(df7)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:976: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for TF-IDF:\n",
      "Logistic Regression Percent Correct:  91.1214953271028\n",
      "K Nearest Neighbor Percent Correct:  90.18691588785047\n",
      "Gaussian Naive Bayes Percent Correct:  77.57009345794393\n",
      "Support Vector Machine Percent Correct:  92.99065420560748\n",
      "Linear SVC Percent Correct:  82.94392523364486\n",
      "Linear Discriminant Analysis Percent Correct:  90.88785046728972\n",
      "Multi-Layer Perceptron Percent Correct:  92.05607476635514\n",
      "\n",
      "\n",
      "Accuracy of all of the above combined:  92.99065420560748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-cb3b58ae2706>:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combodf['sum'] = sumdf\n",
      "<ipython-input-1-cb3b58ae2706>:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  combodf['sumpreds'] = np.where(combodf['sum'] >= 3, True, False)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF: unigram, bigram, and trigram\n",
    "#Same code as above with some tf-idf stuff in it\n",
    "#Again, may want to restart kernel before running\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#open file\n",
    "train = pd.read_csv(r'train.csv')\n",
    "\n",
    "#Look at first 5 lines of data:\n",
    "#train.head()\n",
    "\n",
    "#Merge x text and y text together so I can put them in a matrix\n",
    "train['descriptions'] = train['description_x'].str.cat(train['description_y'],sep=\" \")\n",
    "\n",
    "#making vectors\n",
    "count_vectorizer = CountVectorizer(ngram_range = (1, 1))\n",
    "#above line: 1, 1 = unigram, 2, 2 = bigram, 3, 3, = trigram\n",
    "#Note: restart kernel and run again when changing\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "tokenlinelist = train['descriptions'][0:1713] #split data -- ratio 80:20\n",
    "count_matrix = count_vectorizer.fit_transform(tokenlinelist) #bag of words\n",
    "X_train = tfidf_transformer.fit_transform(count_matrix) #TF-IDF\n",
    "y_train = train['same_security'][0:1713]\n",
    "\n",
    "#split test set\n",
    "tokenlinelist_test = train['descriptions'][1714:2142]\n",
    "count_matrix_test = count_vectorizer.transform(tokenlinelist_test)\n",
    "X_test = tfidf_transformer.fit_transform(count_matrix_test)\n",
    "y_test = train['same_security'][1714:2142]\n",
    "\n",
    "#Logistic Regresion:\n",
    "#train my LR\n",
    "clf = LogisticRegression().fit(X_train, y_train)\n",
    "#Make LR predictions\n",
    "LRpreds = clf.predict(X_test)\n",
    "#print(LRpreds)\n",
    "\n",
    "#KNN classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=2)\n",
    "neigh.fit(X_train, y_train)\n",
    "\n",
    "knc = neigh.predict(X_test)\n",
    "#knc\n",
    "\n",
    "#Gaussian Naive Bayes Classifier\n",
    "gnb = GaussianNB()\n",
    "gnbpreds = gnb.fit(X_train.toarray(), y_train).predict(X_test.toarray())\n",
    "#gnbpreds\n",
    "\n",
    "#SVM Classifier:\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "svmpreds = svm.predict(X_test)\n",
    "\n",
    "#LinearSVC\n",
    "lsvc = make_pipeline(StandardScaler(with_mean = False), LinearSVC(random_state=0, tol=1e-5)) \n",
    "#had to add with_mean = False because I'm working with sparse matrix\n",
    "lsvc.fit(X_train, y_train)\n",
    "\n",
    "lsvcpreds = lsvc.predict(X_test)\n",
    "\n",
    "#Linear Discriminant Analysis\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train.toarray(), y_train) #had to add .toarray() because sparse matrix\n",
    "\n",
    "ldapreds = lda.predict(X_test)\n",
    "\n",
    "#Multi-Layer Perceptron \n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=333)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "mlppreds = mlp.predict(X_test)\n",
    "\n",
    "#Wrangling predictions into one dataframe\n",
    "#Contains test set true classes and my various predicted classes\n",
    "predsdf = pd.DataFrame(LRpreds)\n",
    "truedf = pd.DataFrame(y_test)\n",
    "truedf = truedf.reset_index()\n",
    "knndf = pd.DataFrame(knc)\n",
    "gnbdf = pd.DataFrame(gnbpreds)\n",
    "lsvcdf = pd.DataFrame(lsvcpreds)\n",
    "predsdf['true'] = truedf['same_security']\n",
    "predsdf = predsdf.rename(columns={0: 'LRpreds'})\n",
    "knndf = knndf.rename(columns={0:'KNNpreds'})\n",
    "svmdf = pd.DataFrame(svmpreds)\n",
    "ldadf = pd.DataFrame(ldapreds)\n",
    "mlpdf = pd.DataFrame(mlppreds)\n",
    "predsdf['LRcorrect'] = predsdf['LRpreds'] == predsdf['true']\n",
    "predsdf['KNNpreds'] = knndf\n",
    "predsdf['KNNcorrect'] = predsdf['KNNpreds'] == predsdf['true']\n",
    "predsdf['GNBpreds'] = gnbdf\n",
    "predsdf['GNBcorrect'] = predsdf['GNBpreds'] == predsdf['true']\n",
    "predsdf['SVMpreds'] = svmdf\n",
    "predsdf['SVMcorrect'] = predsdf['SVMpreds'] == predsdf['true']\n",
    "predsdf['LinearSVCpreds'] = lsvcdf\n",
    "predsdf['LinearSVCcorrect'] = predsdf['LinearSVCpreds'] == predsdf['true']\n",
    "predsdf['LDApreds'] = ldadf\n",
    "predsdf['LDAcorrect'] = predsdf['LDApreds'] == predsdf['true']\n",
    "predsdf['MLPpreds'] = mlpdf\n",
    "predsdf['MLPcorrect'] = predsdf['MLPpreds'] == predsdf['true']\n",
    "#predsdf.head()\n",
    "\n",
    "#combining all prediction measures together:\n",
    "#If more than half the measures say true, then make a true prediction; same applies to false\n",
    "combodf = predsdf[['KNNpreds', 'GNBpreds', 'SVMpreds', 'LinearSVCpreds', 'LDApreds', 'MLPpreds']]\n",
    "combosumpreds = combodf.sum(axis=1)\n",
    "sumdf = pd.DataFrame(combosumpreds)\n",
    "combodf['sum'] = sumdf\n",
    "combodf['sumpreds'] = np.where(combodf['sum'] >= 3, True, False)\n",
    "#combodf\n",
    "df7 = combodf.join(predsdf['true'])\n",
    "df7['sumpredscorrect'] = df7['sumpreds'] == df7['true']\n",
    "#df7\n",
    "#print(df7['sumpredscorrect'])\n",
    "\n",
    "\n",
    "print('Results for TF-IDF:')\n",
    "print(\"Logistic Regression Percent Correct: \", predsdf.LRcorrect.sum()/len(predsdf)*100)\n",
    "print(\"K Nearest Neighbor Percent Correct: \", predsdf.KNNcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Gaussian Naive Bayes Percent Correct: \", predsdf.GNBcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Support Vector Machine Percent Correct: \", predsdf.SVMcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Linear SVC Percent Correct: \", predsdf.LinearSVCcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Linear Discriminant Analysis Percent Correct: \", predsdf.LDAcorrect.sum()/len(predsdf)*100)\n",
    "print(\"Multi-Layer Perceptron Percent Correct: \", predsdf.MLPcorrect.sum()/len(predsdf)*100)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy of all of the above combined: \", df7.sumpredscorrect.sum()/len(df7)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 512)          2560000   \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 51200)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 307206    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 2,867,213\n",
      "Trainable params: 2,867,213\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "41/41 [==============================] - 2s 40ms/step - loss: 0.5318 - accuracy: 0.7463 - val_loss: 0.3851 - val_accuracy: 0.7635\n",
      "Epoch 2/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.3468 - accuracy: 0.8498 - val_loss: 0.3082 - val_accuracy: 0.8970\n",
      "Epoch 3/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.2821 - accuracy: 0.9113 - val_loss: 0.2906 - val_accuracy: 0.9087\n",
      "Epoch 4/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.2405 - accuracy: 0.9354 - val_loss: 0.2838 - val_accuracy: 0.8689\n",
      "Epoch 5/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.2149 - accuracy: 0.9463 - val_loss: 0.2682 - val_accuracy: 0.9180\n",
      "Epoch 6/20\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 0.1953 - accuracy: 0.9580 - val_loss: 0.2674 - val_accuracy: 0.9157\n",
      "Epoch 7/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.1851 - accuracy: 0.9720 - val_loss: 0.2789 - val_accuracy: 0.8970\n",
      "Epoch 8/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1755 - accuracy: 0.9774 - val_loss: 0.3063 - val_accuracy: 0.8876\n",
      "Epoch 9/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1680 - accuracy: 0.9767 - val_loss: 0.2789 - val_accuracy: 0.9087\n",
      "Epoch 10/20\n",
      "41/41 [==============================] - 1s 34ms/step - loss: 0.1614 - accuracy: 0.9829 - val_loss: 0.2784 - val_accuracy: 0.9087\n",
      "Epoch 11/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1552 - accuracy: 0.9829 - val_loss: 0.2977 - val_accuracy: 0.9040\n",
      "Epoch 12/20\n",
      "41/41 [==============================] - 1s 31ms/step - loss: 0.1497 - accuracy: 0.9852 - val_loss: 0.3147 - val_accuracy: 0.9016\n",
      "Epoch 13/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1451 - accuracy: 0.9883 - val_loss: 0.3230 - val_accuracy: 0.8993\n",
      "Epoch 14/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1424 - accuracy: 0.9837 - val_loss: 0.2956 - val_accuracy: 0.9110\n",
      "Epoch 15/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1378 - accuracy: 0.9868 - val_loss: 0.3224 - val_accuracy: 0.8993\n",
      "Epoch 16/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1340 - accuracy: 0.9860 - val_loss: 0.2967 - val_accuracy: 0.9133\n",
      "Epoch 17/20\n",
      "41/41 [==============================] - 1s 33ms/step - loss: 0.1297 - accuracy: 0.9883 - val_loss: 0.3224 - val_accuracy: 0.9087\n",
      "Epoch 18/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1265 - accuracy: 0.9875 - val_loss: 0.3081 - val_accuracy: 0.9063\n",
      "Epoch 19/20\n",
      "41/41 [==============================] - 1s 31ms/step - loss: 0.1238 - accuracy: 0.9883 - val_loss: 0.3113 - val_accuracy: 0.9087\n",
      "Epoch 20/20\n",
      "41/41 [==============================] - 1s 32ms/step - loss: 0.1213 - accuracy: 0.9883 - val_loss: 0.3180 - val_accuracy: 0.9087\n",
      "Tensorflow RNN Percent Correct:  92.28971962616822\n"
     ]
    }
   ],
   "source": [
    "#Tensor Flow RNN\n",
    "#https://github.com/MGCodesandStats/tensorflow-nlp/blob/master/spam%20detection%20tensorflow%20v1.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "dataset = pd.read_csv(r'train.csv')\n",
    "\n",
    "dataset['descriptions'] = dataset['description_x'].str.cat(dataset['description_y'],sep=\" \")\n",
    "\n",
    "sentences = dataset['descriptions'].tolist()\n",
    "labels = dataset['same_security'].tolist() # Separate out the sentences and labels into training and validation sets\n",
    "training_sentences = sentences[0:1285]\n",
    "testing_sentences = sentences[1286:1713]\n",
    "training_labels = labels[0:1285]\n",
    "testing_labels = labels[1286:1713] # Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n",
    "\n",
    "#tweaked these several times; these settings kind of got the best accuracy\n",
    "vocab_size = 5000 \n",
    "embedding_dim = 512\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "#tokenizing and making vectors\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "\n",
    "padded = pad_sequences(sequences,maxlen=max_length, padding=padding_type,  truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences,maxlen=max_length, \n",
    "                               padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "#making the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#10 and 30 epochs did worse\n",
    "num_epochs = 20\n",
    "history=model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))\n",
    "\n",
    "# Create the sequences\n",
    "pred_sentences = sentences[1714:2142]\n",
    "\n",
    "\n",
    "# Create the sequences\n",
    "padding_type='post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(pred_sentences)\n",
    "fakes_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_length)           \n",
    "\n",
    "#make prediction\n",
    "pred_classes = model.predict(fakes_padded)\n",
    "\n",
    "# The closer the class is to 1, the more likely that the message is spam\n",
    "tfrnndf = pd.DataFrame(pred_classes)\n",
    "tfrnndf['TFRNNpreds'] = np.where(tfrnndf[0] >= .9, True, False)\n",
    "tfrnndf['TFRNNcorrect'] = tfrnndf['TFRNNpreds'] == predsdf['true'] #FIX so it works independently\n",
    "print(\"Tensorflow RNN Percent Correct: \", tfrnndf.TFRNNcorrect.sum()/len(tfrnndf)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
